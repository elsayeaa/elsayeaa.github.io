---
layout: post
title: Week 2 
---
# General Experience 
This week has been better than the last week. I have continued working on the empirical experiment to prove the maths I have worked out. I have finished beginner tutorials for [PyTorch](https://pytorch.org/tutorials/), and I have read through [causal graphical models](https://github.com/ijmbarr/causalgraphicalmodels) and [the causal discovery algorithm](https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html). I have also reviewed neural networks and the mathematics behind it. I have learned a lot from designing my first empirical experiements, and I have reviewed by Python and NumPy coding skills. Overall, it has been a very enriching week, and I am excited for next week! 
# Readings (Litrature Review and online blogposts and tutorials)
I was also still reading [Invariant Models for Causal Transfer Learning](https://www.jmlr.org/papers/volume19/16-432/16-432.pdf) paper, mainly focusing on proposition 2 and how to reapply it using different conditions. I have also used causal inference blogposts and reviewed my probability theory knowledge regarding expectations and independence. Overall, not so much reading this week but mostly mechanical work. 
# Experiments & Algorithms 
## Math and conceptual understanding
I have been tasked with a problem this week. Basically, for a structural causal model, we have X, Y, Z. We want to find the conditions where the anticausal model, that is a model depending on ```X and Z: (X,Z)->Y```, will perform better than the causal model, a model only depnding on the causal random variable ```X: (X)->(Z)```. Here is a snip of the equations that we have: 
![image](https://user-images.githubusercontent.com/64815927/121470625-59350100-c973-11eb-8e18-6b34fe8b5774.png)

This what we are looking for: 

![image](https://user-images.githubusercontent.com/64815927/121469750-e5462900-c971-11eb-8b4c-a73a35573b9f.png) 

This basically tells us that we need to keep the influence of the exogenous factors for the transfer model (eta transfer) less than the influence of the exogenous factors of the training model and the internal biases within our model. You can follow the math through this [link](https://drive.google.com/file/d/11u-05aTLEpzOTGAyuQwsDpJCS7a3gaYh/view?usp=sharing). Also, most of the mathematical foundations where based on [this paper](https://www.jmlr.org/papers/volume19/16-432/16-432.pdf). 
## Experiement using [Jupyter notebooks](https://jupyter.org/) 
For this experiement, I have followed these steps: 
1. I generated a training dataset, with randomly generated data distributions of X, Y, and Z. I have also generated random weights. 
2. Then, I generated a test dataset, with random weights for their data distributions (so we have source and test data coming from different data distributions). 
3. I have created 2 arrays: the first has 200 different values for eta source ranging from 0.01 to 1.99, the second does the same thing for eta transfer. 
4. I have calculated the expected loss using a mean squared error function, i.e. MSE, for both the causal and the anticausal model when varying the etas. 
5. I have plotted the different values of MSE for both the causal and anticausal models with respect to the eta.
6. Then, I have observed the behavior of the error as we increase the etas and how the math proves its workability.

# Results and Findings 
## Experiment results
After following the testing algorithm and plotting the curves, these are some of the plots that I got. 
![image](https://user-images.githubusercontent.com/64815927/121471468-977ef000-c974-11eb-9eaa-cf31310ce064.png)
![image](https://user-images.githubusercontent.com/64815927/121471507-a82f6600-c974-11eb-9b72-252a3b42ab1b.png)
![image](https://user-images.githubusercontent.com/64815927/121471542-b9787280-c974-11eb-973a-e340435c47cd.png)

So we can see that when we increase the eta transfer, the anticausal error becomes higher than the causal error. Consequently, making the anticausal model worse and riskier than the causal model. However, we interested in the period where the anticausal model performs better than the causal model. 

## General Findings
* Through reading and discussing with my mentor, I have found that in certain conditions, using the anticausal model can produce better results than the causal model; however, we need to devise a method that can tell a system which model to follow. This can be done through an analysis of errors and investigation of how well each model performs. 
* Causal relationship assume independence between random variables; however, anticausal relationships do not assume so. Therefore, we have exogenous factors that influence our error. The problem is that we don't see these exogenous factors, and in some datasets, we are not told whether a random variable is causal or anticausal. 
* TODO 
# Frustrations 
My frustrations this week were miniscule. The biggest out of them was debugging my Python code and getting used to using NumPy and matplotlib again. I guess this problem will continue until week 4 or 5. By then, I will have written enough code to utilize them smoothly without going back and forth from my code to the documentation. 
# Plans for Next Week 
Our plans for next week is to read this article: [Review of Causal Discovery Methods Based on Graphical Models](https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full). Then, we will develop an experimental framwork to:
1. Generate data from a given Structural Causal Model (SCM).
2. Apply a causal discovery algorithm to generated a mixed graph. 
3. Exrract parents and children of outcome variable from graph.
4. Generate distinct DataFrames for parents, outcome variable, and children. 
5. Learn a predictive model given a specified learning framework. 
6. Analyze the statistical daata from given target datasets. 

# Other information worth sharing 


